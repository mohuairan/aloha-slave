# pi0.5 åœ¨çº¿è®­ç»ƒä¸ ALOHA å®ç‰©ç­–ç•¥éƒ¨ç½²æŒ‡å—ï¼ˆGoogle Colab A100, å•å¡ 40GB / Colab Pro 10$ ç‰ˆï¼‰  

> ç®€ä»‹ï¼šæœ¬é¡¹ç›®è¯´æ˜å¦‚ä½•åœ¨ Google Colabï¼ˆå•å¼  A100ï¼Œ40GBï¼‰ä¸Šè¿›è¡Œ **pi0.5ï¼ˆpi05ï¼‰åœ¨çº¿å¾®è°ƒ** å¹¶å°†è®­ç»ƒå¾—åˆ°çš„ç­–ç•¥éƒ¨ç½²åˆ° ALOHA æœºæ¢°è‡‚çš„å®æœºä¸Šã€‚æ ¸å¿ƒæµç¨‹åŒ…å«ï¼šå‰ç½®å‡†å¤‡ â†’ ç¯å¢ƒæ­å»º â†’ æ•°æ®é›†å¤„ç† â†’ æ¨¡å‹é…ç½®ä¸å½’ä¸€åŒ– â†’ è®­ç»ƒ â†’ Colab ä¿æ´»ä¸å¤‡ä»½ â†’ æ¨¡å‹ä¸‹è½½ â†’ å®æœºéƒ¨ç½²ï¼ˆHost/Clientï¼‰â†’ å¸¸è§é—®é¢˜æ’æŸ¥ã€‚  
> æœ¬æ–‡æ¡£åŸºäºç”¨æˆ·æä¾›çš„å·¥ä½œæµç¬”è®°æ•´ç†ä¸è¡¥å……ã€‚

---

## ç›®å½•
- [å‰ç½®å‡†å¤‡](#å‰ç½®å‡†å¤‡)  
- [ä»“åº“å…‹éš†ä¸å­æ¨¡å—](#ä»“åº“å…‹éš†ä¸å­æ¨¡å—)  
- [å®‰è£… uv ä¸ OpenPI ä¾èµ–ï¼ˆç¯å¢ƒæ­å»ºï¼‰](#å®‰è£…-uv-ä¸-openpi-ä¾èµ–ç¯å¢ƒæ­å»º)  
- [æ•°æ®é›†å¤„ç†ï¼ˆHugging Faceï¼‰](#æ•°æ®é›†å¤„ç†hugging-face)

---

## å‰ç½®å‡†å¤‡

- **æ³¨å†Œå¹¶ç™»å½• Hugging Face**ï¼šåˆ›å»ºæ•°æ®é›†ä»“åº“ä¸æ¨¡å‹ä»“åº“ã€‚**ä¿å­˜å¥½ Hugging Face tokenï¼ˆAPI keyï¼‰â€”â€”éå¸¸é‡è¦ï¼**  
  ```bash
  huggingface-cli login
  huggingface-cli whoami
  ```
- **ä»“åº“ç»“æ„è¦æ±‚ï¼ˆæé‡è¦ï¼‰**ï¼šä¸Šä¼ åˆ° HF çš„ dataset repo è¿›å…¥å**å¿…é¡»**ç›´æ¥åŒ…å« `data/` ä¸ `meta/` æ–‡ä»¶å¤¹ï¼ˆ**ä¸èƒ½**å°†æ•°æ®æ”¾åœ¨å­æ–‡ä»¶å¤¹é‡Œï¼‰ã€‚**å¼ºçƒˆæé†’ï¼šè¿›ä»“åº“ä¹‹åç›´æ¥å°±æ˜¯ data ä¸ meta æ–‡ä»¶å¤¹ï¼Œä¸å¯ä»¥æ˜¯å…¶ä»–ç»“æ„ã€‚**

---

## ä»“åº“å…‹éš†ä¸å­æ¨¡å—

å»ºè®®å…‹éš†æ—¶ä½¿ç”¨ `--recurse-submodules`ï¼Œå¦‚æœå·²å…‹éš†éœ€åˆå§‹åŒ–å­æ¨¡å—ã€‚

```bash
git clone --recurse-submodules https://github.com/Physical-Intelligence/openpi.git
cd openpi

# å¦‚æœå·²ç»å…‹éš†ï¼š
git submodule update --init --recursive
```

---

## å®‰è£… uv ä¸ OpenPI ä¾èµ–ï¼ˆç¯å¢ƒæ­å»ºï¼‰

OpenPI ä½¿ç”¨ [uv](https://astral.sh/uv/) ç®¡ç†ä¾èµ–ã€‚ä»¥ä¸‹ä¸º Colab / æœ¬åœ°ï¼ˆUbuntuï¼‰å¸¸ç”¨æ­¥éª¤ã€‚

### å®‰è£… uvï¼ˆcurl å®‰è£…è„šæœ¬ï¼‰
```bash
# å®‰è£… uv
curl -LsSf https://astral.sh/uv/install.sh | sh
# ä½¿ cargo / ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ
source $HOME/.cargo/env

# éªŒè¯
uv --version
```

### å®‰è£… OpenPI ä¾èµ–ï¼ˆæ³¨æ„ GIT_LFS_SKIP_SMUDGEï¼‰
`GIT_LFS_SKIP_SMUDGE=1` åœ¨æ‹‰å–å¸¦å¤§æ–‡ä»¶ä¾èµ–ï¼ˆLeRobot ç­‰ï¼‰æ—¶æ˜¯å¿…è¦çš„ï¼š

```bash
# åŒæ­¥ä¾èµ–ï¼ˆè·³è¿‡ LFS smudgeï¼‰
GIT_LFS_SKIP_SMUDGE=1 uv sync

# å®‰è£…åŒ…ï¼ˆå¯ç¼–è¾‘å®‰è£…ï¼‰
GIT_LFS_SKIP_SMUDGE=1 uv pip install -e .
```

**æ³¨æ„**ï¼šuv å¯èƒ½ä¼šæç¤º `tool.uv.dev-dependencies` çš„å¼ƒç”¨è­¦å‘Šï¼ˆå¯å¿½ç•¥æˆ–æŒ‰æç¤ºè¿ç§»åˆ° `dependency-groups.dev`ï¼‰ã€‚

---

## æ•°æ®é›†å¤„ç†ï¼ˆHugging Faceï¼‰

### ä½¿ç”¨è„šæœ¬ä¸Šä¼ ï¼ˆå¯é€‰ï¼‰
å¦‚æœæ•°æ®åœ¨æœ¬åœ°ã€å¯ä»¥ä½¿ç”¨å·²æœ‰çš„ `push_to_huggingface.py` ä¸Šä¼ åˆ°ä»“åº“ï¼ˆåŸæ–‡æåˆ°æœ‰è¯¥è„šæœ¬ï¼‰ã€‚ç¡®ä¿ä»“åº“ç»“æ„éµå®ˆï¼šæ ¹ç›®å½•ä¸‹ç›´æ¥ä¸º `data/` ä¸ `meta/`ã€‚

### HF dataset å¿…é¡»æœ‰ç‰ˆæœ¬ tagï¼ˆé¿å… `RevisionNotFoundError`ï¼‰
LeRobot çš„åŠ è½½æœºåˆ¶è¦æ±‚ dataset æœ‰ä»£ç ç‰ˆæœ¬ tagï¼Œå¦åˆ™ä¼šæŠ¥ï¼š
`huggingface_hub.errors.RevisionNotFoundError: Your dataset must be tagged with a codebase version.`

åœ¨ Colab / shell ä¸­ä¸º dataset å¢åŠ  tagï¼ˆç¤ºä¾‹ï¼‰ï¼š

```bash
# ç™»å½•ï¼ˆè‹¥å°šæœªç™»å½•ï¼‰
huggingface-cli login

# åˆ›å»º tagï¼ˆrepo-type = datasetï¼‰
hf repo tag create <USER>/<DATASET_NAME> v1 --repo-type dataset
# ä¾‹å¦‚ï¼š
hf repo tag create mo0821/aloha_training_pick_tissue_2 v1 --repo-type dataset
```

æˆ–è€…ç”¨ Python APIï¼š
```python
from huggingface_hub import HfApi
hub_api = HfApi()
hub_api.create_tag("mo0821/aloha_training_pick_tissue_2", tag="_version_", repo_type="dataset")
```

### å¦‚æœè‡ªåŠ¨åŠ è½½å¤±è´¥ï¼šæ‰‹åŠ¨ä¸‹è½½ dataset åˆ°æœ¬åœ°ç¼“å­˜ç›®å½•
æœ‰æ—¶ compute_norm_stats æˆ–æ•°æ®åŠ è½½ä¼šæç¤ºç¼ºå°‘ `meta/info.json`ï¼ˆæˆ– dataset æœªè¢«è‡ªåŠ¨æ‹‰å–ï¼‰ï¼Œå¯æ‰‹åŠ¨ä¸‹è½½åˆ° Hugging Face æœ¬åœ°ç¼“å­˜è·¯å¾„ï¼š

```bash
# åˆ›å»ºç›®æ ‡ç›®å½•
mkdir -p /root/.cache/huggingface/lerobot/mo0821/aloha_test

# ä¸‹è½½ dataset åˆ°æŒ‡å®šç›®å½•ï¼ˆä¸¤ç§å‘½ä»¤ä»»é€‰å…¶ä¸€ï¼‰
huggingface-cli download mo0821/aloha_test --repo-type dataset --local-dir /root/.cache/huggingface/lerobot/mo0821/aloha_test

# æˆ–è€…ï¼ˆæ–°å·¥å…·ï¼‰
hf download mo0821/aloha_test --repo-type dataset --local-dir /root/.cache/huggingface/lerobot/mo0821/aloha_test
```
---

## æ¨¡å‹é…ç½®ä¸å½’ä¸€åŒ–ï¼ˆTrainConfig / å½’ä¸€åŒ–ç»Ÿè®¡ï¼‰

è¿›å…¥ `openpi/src/openpi/training/`ï¼Œåœ¨ `config` çš„ `TrainConfig` ä¸­æ·»åŠ /ä¿®æ”¹å¯¹åº”ä»»åŠ¡é…ç½®ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ª **LoRA å¾®è°ƒï¼ˆpi0.5ï¼‰** çš„ç¤ºä¾‹é…ç½®ï¼ˆ**æ³¨æ„æ˜¾å­˜è¦æ±‚ï¼š22.5GB+**ï¼‰ï¼š

```python
# ç¤ºä¾‹ï¼šTrainConfigï¼ˆæ”¾åœ¨ç›¸åº”çš„ config æ–‡ä»¶ä¸­ï¼‰
TrainConfig(
    name="pi05_aloha_pick_tissue_finetune",
    model=pi0_config.Pi0Config(
        paligemma_variant="gemma_2b_lora", 
        action_expert_variant="gemma_300m_lora",
        pi05=True
    ),
    data=LeRobotAlohaDataConfig(
        repo_id="mo0821/aloha_test",
        assets=AssetsConfig(
            assets_dir="/root/.cache/pi05_base/assets",
            asset_id="trossen",
        ),
        default_prompt="pick up the tissue and put it into box",
        repack_transforms=_transforms.Group(
            inputs=[
                _transforms.RepackTransform(
                    {
                        "images": {
                            "cam_high": "observation.images.camera_high",
                            "cam_left_wrist": "observation.images.camera_wrist_left",
                            "cam_right_wrist": "observation.images.camera_wrist_right",
                        },
                        "state": "observation.state",
                        "actions": "action",
                    }
                )
            ]
        ),
    ),
    weight_loader=weight_loaders.CheckpointWeightLoader("/root/.cache/pi05_base/params"),
    num_train_steps=20_000,
    freeze_filter=pi0_config.Pi0Config(
        paligemma_variant="gemma_2b_lora", 
        action_expert_variant="gemma_300m_lora",
        pi05=True
    ).get_freeze_filter(),
    ema_decay=None,  # LoRAå¾®è°ƒæ—¶å…³é—­EMA
),
```

> **é‡ç‚¹**ï¼š
> - `paligemma_variant="gemma_2b_lora"` ä¸ `action_expert_variant="gemma_300m_lora"`ã€`pi05=True` ç­‰å­—æ®µéœ€ç¡®ä¿ä¸€è‡´ä¸”æ­£ç¡®ã€‚  
> - `assets_dir` å’Œ `weight_loader` è·¯å¾„éœ€è¦æŒ‡å‘ä½ å·²ä¸‹è½½/å‡†å¤‡å¥½çš„ `pi05_base` èµ„æ–™ã€‚éƒ¨ç½²/serve æ—¶å»ºè®®æŠŠ `assets` æ”¾åœ¨é rootã€å¯è®¿é—®çš„ç›®å½•ï¼ˆåœ¨ Host/Client ä¸­ä¼šæåˆ°ï¼‰ã€‚

### ç”Ÿæˆå½’ä¸€åŒ–ç»Ÿè®¡ï¼ˆcompute_norm_statsï¼‰
åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œï¼š

```bash
uv run scripts/compute_norm_stats.py --config-name pi05_aloha_pick_tissue_finetune
```

**å¦‚æœå‡ºç°æ•°æ®åŠ è½½é”™è¯¯**ï¼ˆå¦‚ `FileNotFoundError: ... meta/info.json` æˆ– `RevisionNotFoundError`ï¼‰ï¼Œè¯·å…ˆæŒ‰ä¸ŠèŠ‚æ‰‹åŠ¨ä¸‹è½½ dataset å¹¶åˆ›å»º tagã€‚æ›´å¤šé”™è¯¯è§ FAQã€‚

---

## æ¨¡å‹è®­ç»ƒ

åœ¨ Colabï¼ˆæˆ–å·²é…ç½®å¥½ JAX/XLA çš„ç¯å¢ƒï¼‰ä¸­è¿è¡Œè®­ç»ƒï¼š

```bash
# ä¾‹å¦‚è®¾ç½® XLA å†…å­˜åˆ†é…ï¼Œç„¶åè¿è¡Œè®­ç»ƒ
XLA_PYTHON_CLIENT_MEM_FRACTION=0.99 uv run scripts/train.py pi05_aloha_pick_tissue_finetune --exp-name=my_experiment --overwrite
```

- `--exp-name` ç”¨äº checkpoint å‘½åç©ºé—´ã€‚  
- `--overwrite` å¯è¦†ç›–å·²å­˜åœ¨çš„åŒåå®éªŒã€‚  
- æ ¹æ®ä»»åŠ¡å¤æ‚åº¦ä¸æ˜¾å­˜ï¼Œé€‚å½“è°ƒæ•´ `num_train_steps`ã€batch size ç­‰ã€‚

---

## Colab ä¿æ´»ä¸è‡ªåŠ¨å¤‡ä»½è„šæœ¬ï¼ˆç¤ºä¾‹ï¼‰

> **è¯´æ˜**ï¼šColab å®¹æ˜“æ–­å¼€ã€‚ä¸‹é¢ç»™å‡ºä¸¤ä¸ªè„šæœ¬ç¤ºä¾‹ï¼šä¸€ä¸ªç”¨äºå®šæ—¶æŠŠ checkpoints åŒæ­¥åˆ° Google Drive çš„å¤‡ä»½è„šæœ¬ï¼›å¦ä¸€ä¸ªæ˜¯ç®€å•çš„â€œä¿æ´»â€çº¿ç¨‹ï¼Œå®šæœŸè¾“å‡ºä¿¡æ¯é˜²æ­¢ç©ºé—²æ–­å¼€ï¼ˆä¸ä¿è¯ 100% æœ‰æ•ˆï¼Œéœ€é…åˆæµè§ˆå™¨ç«¯é•¿æœŸä¿æŒæ´»åŠ¨ï¼‰ã€‚éœ€è¦ä¿è¯ä½ çš„ Google Drive æœ‰è¶³å¤Ÿç©ºé—´ã€‚

### 1) å®šæ—¶å¤‡ä»½ï¼ˆPythonï¼‰
```python
# Colab: å®šæ—¶å¤‡ä»½ checkpoint åˆ° Google Drive
import time
from datetime import datetime
import shutil
import os

source_folder = "/content/openpi/checkpoints/pi05_aloha_pick_tissue_finetune"
backup_folder = "/content/drive/MyDrive/openpi_checkpoints_backup"

os.makedirs(backup_folder, exist_ok=True)

while True:
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[Heartbeat] Colab active at {now}", flush=True)
    
    # æ¯éš”30åˆ†é’Ÿå¤‡ä»½ä¸€æ¬¡ï¼ˆç”¨æ—¶é—´æˆ³ç®€å•æ§åˆ¶ï¼‰
    if int(time.time()) % (30 * 60) < 10:
        backup_path = os.path.join(backup_folder, f"backup_{now.replace(':','-')}")
        shutil.copytree(source_folder, backup_path, dirs_exist_ok=True)
        print(f"[Backup] Saved checkpoint to {backup_path}", flush=True)
    
    time.sleep(600)  # æ¯10åˆ†é’Ÿæ‰“å°ä¸€æ¬¡
```

### 2) Python ä¿æ´»ï¼ˆåå°çº¿ç¨‹ï¼‰
```python
# Colab: ä¿æ´»çº¿ç¨‹ï¼ˆè¾“å‡ºæ¶ˆæ¯ä»¥ä¿æŒ notebook æ´»åŠ¨ï¼‰
import threading
import time
import random

def keep_alive_python():
    messages = [
        "ğŸ”„ Colab ä¿æŒæ´»è·ƒä¸­...",
        "ğŸ“Š æ­£åœ¨å¤„ç†æ•°æ®...",
        "ğŸ¤– AI è®­ç»ƒè¿›è¡Œä¸­...",
        "âš¡ è®¡ç®—ä¸­ï¼Œè¯·å‹¿æ–­å¼€...",
        "ğŸ”¬ å®éªŒè¿è¡Œä¸­..."
    ]
    while True:
        print(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {random.choice(messages)}")
        time.sleep(55)  # å°äº60ç§’ä»¥å…è¶…æ—¶

thread = threading.Thread(target=keep_alive_python, daemon=True)
thread.start()
print("Python ä¿æŒæ´»è·ƒçº¿ç¨‹å·²å¯åŠ¨")
```

> **æ³¨æ„**ï¼šé•¿æœŸä¾èµ–æ­¤ç±»è„šæœ¬å¹¶éå®˜æ–¹ä¿è¯æ–¹æ³•ï¼›åŠ¡å¿…ç»å¸¸æŠŠé‡è¦ checkpoint å¤‡ä»½åˆ° Drive æˆ–å¤–éƒ¨å­˜å‚¨ã€‚

---

## æ¨¡å‹æ‰“åŒ…ä¸ä¸‹è½½ï¼ˆè°·æ­Œäº‘ç›˜ / gdown / gsutilï¼‰

### åœ¨ Colab ä¸­å‹ç¼©æœ€ç»ˆ checkpointï¼ˆç¤ºä¾‹ï¼‰
å»ºè®®åœ¨ Colab ä¸­æŠŠç›®æ ‡ checkpoint å‹ç¼©æˆä¸€ä¸ª zip æ–‡ä»¶å†åŒæ­¥åˆ° Google Driveï¼Œé¿å…ä¸‹è½½ä¸å®Œæ•´æˆ–è¢«åˆ‡å‰²æˆå¤šä¸ªåŒ…ã€‚

```bash
# Colab ç¤ºä¾‹ï¼šå°†æŸä¸ª step çš„ checkpoint å‹ç¼©
!zip -r /content/checkpoint/pi05_aloha_pick_tissue_finetune_19999.zip       /content/drive/MyDrive/openpi/checkpoints/pi05_aloha_pick_tissue_finetune/my_experiment/19999
```

### åœ¨æœ¬åœ°ä½¿ç”¨ gdown ä¸‹è½½ï¼ˆå½“ä½ æŠŠ zip æ”¾åˆ° Google Drive å¹¶å…¬å¼€å¯è§ï¼‰
1. åœ¨ Google Drive ä¸­æŠŠå¯¹åº”æ–‡ä»¶/æ–‡ä»¶å¤¹è®¾ç½®ä¸ºâ€œä»»ä½•äººæœ‰é“¾æ¥å¯è§â€ï¼ˆæ³¨æ„éšç§é£é™©ï¼‰ã€‚  
2. ä»åˆ†äº«é“¾æ¥æå– `id`ï¼ˆshare url ä¸­ `drive/folders/<ID>` æˆ– `id` å‚æ•°ï¼‰ã€‚  
3. ä½¿ç”¨ `gdown`ï¼ˆå®‰è£…ï¼š`pip install gdown`ï¼‰ä¸‹è½½ï¼š

```bash
# ç¤ºä¾‹ï¼ˆid ä¸ºåˆ†äº«é“¾æ¥é‡Œçš„ IDï¼‰
gdown https://drive.google.com/uc?id=1812aDQgMYlkjo7cyLWaT0Lxao4c0dYp8 -O /home/you/aloha_data/checkpoint/
```

### å¦‚æœä½¿ç”¨ Google Cloud Storageï¼ˆgsutilï¼‰
éƒ¨åˆ† checkpoint å­˜å‚¨åœ¨ GCSï¼ˆä¾‹å¦‚ `gs://openpi-assets/checkpoints/pi05_base`ï¼‰ï¼Œè‹¥ `pi05_base` æ–‡ä»¶å¤¹ä¸ºç©º/ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ï¼š

```bash
# å®‰è£… Google Cloud SDKï¼ˆç¤ºä¾‹ï¼‰
curl https://sdk.cloud.google.com | bash
# å®‰è£… crcmodï¼ˆé¿å…ä¸‹è½½å¤±è´¥ï¼‰
sudo apt-get install python3-dev gcc -y
pip install -U crcmod

# ä½¿ç”¨ gsutil ä¸‹è½½ï¼ˆç¤ºä¾‹ï¼‰
cd ~/.cache
gsutil -m cp -r gs://openpi-assets/checkpoints/pi05_base ./
```

> **æ³¨æ„**ï¼šè‹¥æ²¡æœ‰æƒé™æˆ–æ–‡ä»¶ä¸å­˜åœ¨ï¼Œgsutil ä¼šæŠ¥é”™ï¼›ç¡®è®¤è·¯å¾„ä¸æƒé™ã€‚

---

## å®æœºç­–ç•¥éƒ¨ç½²ï¼ˆHostï¼šè¿œç¨‹æ¨ç† / Clientï¼šå®ç‰©æ§åˆ¶ï¼‰

æ•´ä½“è®¾è®¡ï¼šå°†**æ¨ç†ç«¯ï¼ˆHostï¼‰**ä¸**å®æœºæ§åˆ¶ç«¯ï¼ˆClientï¼‰**åˆ†ç¦»ã€‚Host æ‰¿è½½æ¨¡å‹æ¨ç†ï¼ˆéœ€è¾ƒå¤§æ˜¾å­˜ï¼Œä¾‹å¦‚ â‰¥12GBï¼‰ï¼ŒClient è´Ÿè´£ä¸æœºæ¢°è‡‚ / ç›¸æœºç­‰ç¡¬ä»¶é€šä¿¡å¹¶æ¥æ”¶ Host çš„åŠ¨ä½œæŒ‡ä»¤ã€‚

---

### Hostï¼ˆè¿œç¨‹æ¨ç†ï¼‰ â€” ç¯å¢ƒæ­å»ºä¸è¿è¡Œ

1. å…‹éš†ä»“åº“å¹¶è¿›å…¥ï¼š
    ```bash
    git clone https://github.com/Physical-Intelligence/openpi.git
    cd openpi
    ```

2. åˆ›å»º Python 3.11 è™šæ‹Ÿç¯å¢ƒï¼ˆç¤ºä¾‹ï¼‰ï¼š
    ```bash
    python3.11 -m venv .venv
    source .venv/bin/activate
    ```

3. å®‰è£…ä¾èµ–ï¼š
    ```bash
    uv pip install -e .
    ```

4. ä¿®æ”¹ `TrainConfig` ä¸­çš„ `assets` å­—æ®µï¼ˆå°† `assets_dir` æŒ‡å‘ Host ä¸Šå®é™…å¯è¯»çš„è·¯å¾„ï¼‰ï¼š
    ```python
    assets=AssetsConfig(
        assets_dir="/home/you/.cache/pi05_base/assets",  # æ¨èé root è·¯å¾„
        asset_id="trossen",
    )
    ```
    å¹¶æŠŠ `checkpoints/.../assets/trossen` å¤åˆ¶åˆ° `assets_dir` æŒ‡å®šä½ç½®ï¼Œä¿è¯ `norm stats` å¯è¯»ã€‚

5. è¿è¡Œ `serve_policy.py`ï¼ˆç¤ºä¾‹ï¼‰ï¼š
    ```bash
    cd openpi
    python scripts/serve_policy.py --env ALOHA       policy:checkpoint       --policy.config "pi05_aloha_pick_tissue_finetune"       --policy.dir /home/you/serve_train/openpi/checkpoints/pi05_aloha_pick_tissue_finetune/my_experiment/19999
    ```
    æˆåŠŸè¾“å‡ºç¤ºä¾‹ï¼ˆå…³é”®æ—¥å¿—ï¼‰ï¼š
    ```
    INFO:absl:Restoring checkpoint from /home/you/serve_train/openpi/checkpoints/pi05_aloha_pick_tissue_finetune/my_experiment/19999/params.
    INFO:root:Loaded norm stats from /home/you/.cache/pi05_base/assets/trossen
    INFO:root:Creating server (host: your-host, ip: 127.0.1.1)
    INFO:websockets.server:server listening on 0.0.0.0:8000
    ```
    è¡¨ç¤ºæ¨ç†æœåŠ¡å·²å¯åŠ¨å¹¶ç›‘å¬ç«¯å£ï¼ˆé»˜è®¤ 8000ï¼‰ã€‚

---

### Clientï¼ˆå®ç‰©æ§åˆ¶ï¼‰ â€” Docker å¯åŠ¨ä¸è°ƒæ•´

OpenPI å®˜æ–¹æä¾› `examples/aloha_real` ä¸‹çš„ `compose.yml` ä½¿ç”¨ Docker Compose å¯åŠ¨å®ç‰©æ§åˆ¶ç¯å¢ƒã€‚æˆ‘ä»¬æŒ‰éœ€**ç§»é™¤ openpi-server é•œåƒï¼ˆå› ä¸ºæ¨ç†èµ°è¿œç«¯ Hostï¼‰**ï¼Œå¹¶å¯¹ Dockerfile / compose åšè‹¥å¹²ä¿®å¤ã€‚

1. è¿›å…¥ `openpi` æ ¹ç›®å½•ï¼š
    ```bash
    docker compose -f examples/aloha_real/compose.yml up --build
    ```
    - **æ³¨æ„**ï¼šå¦‚æœä½ è¦æŠŠ Host ä¸ Client åˆ†ç¦»ï¼ˆHost åœ¨è¿œç«¯ï¼‰ï¼Œåœ¨ `compose.yml` ä¸­**åˆ é™¤ `openpi-server` ç›¸å…³æœåŠ¡ / depends_on**ã€‚

2. Dockerfile ä¸­çš„ `xsarm_amd64_install.sh` æ‹‰å–è„šæœ¬é‡Œå« `sudo`ï¼Œä¼šå¯¼è‡´ `docker build` å¤±è´¥ï¼ˆå®¹å™¨å†…ä¸åº”ä½¿ç”¨ sudoï¼‰ã€‚**ä¿®å¤æ–¹æ³•ï¼šåœ¨ Dockerfile ä¸­ç”¨ sed åˆ é™¤ sudoã€‚**

```dockerfile
# åœ¨ Dockerfile ä¸­æ·»åŠ ï¼ˆæˆ–åœ¨ build å‰ patch è„šæœ¬ï¼‰
RUN sed -i 's/sudo //g' xsarm_amd64_install.sh
```

3. ä¸ºç¨³å®šè¯†åˆ«æœºæ¢°è‡‚ USB è®¾å¤‡ï¼Œæ·»åŠ  udev è§„åˆ™ï¼ˆç¤ºä¾‹ï¼‰åˆ° Dockerfileï¼š

```dockerfile
# å›ºå®š Interbotix X-Series å››ä¸ªæœºæ¢°è‡‚çš„ USB è®¾å¤‡åç§°
RUN echo "# Interbotix Arm UDEV Rules" >> /etc/udev/rules.d/99-interbotix-arms.rules &&     echo 'SUBSYSTEM=="tty", ATTRS{serial}=="FTAAMN4B", ENV{ID_MM_DEVICE_IGNORE}="1", SYMLINK+="ttyDXL_master_right"' >> /etc/udev/rules.d/99-interbotix-arms.rules &&     echo 'SUBSYSTEM=="tty", ATTRS{serial}=="FTAAMMSJ", ENV{ID_MM_DEVICE_IGNORE}="1", SYMLINK+="ttyDXL_master_left"' >> /etc/udev/rules.d/99-interbotix-arms.rules &&     echo 'SUBSYSTEM=="tty", ATTRS{serial}=="FTAAMN3J", ENV{ID_MM_DEVICE_IGNORE}="1", SYMLINK+="ttyDXL_puppet_left"' >> /etc/udev/rules.d/99-interbotix-arms.rules &&     echo 'SUBSYSTEM=="tty", ATTRS{serial}=="FTA9DPY2", ENV{ID_MM_DEVICE_IGNORE}="1", SYMLINK+="ttyDXL_puppet_right"' >> /etc/udev/rules.d/99-interbotix-arms.rules
```

4. ä¿®æ”¹ `robot_utils.py` ä¸­å¤¹çˆªå‚æ•°ï¼š
```dockerfile
RUN sed -i 's/LEADER_GRIPPER_CLOSE_THRESH = 0.0/LEADER_GRIPPER_CLOSE_THRESH = -1.4/' /root/interbotix_ws/src/aloha/aloha_scripts/robot_utils.py &&     sed -i 's/LEADER_GRIPPER_POSITION_OPEN = 0.0323/LEADER_GRIPPER_POSITION_OPEN = 0.02417/' /root/interbotix_ws/src/aloha/aloha_scripts/robot_utils.py &&     sed -i 's/LEADER_GRIPPER_POSITION_CLOSE = 0.0185/LEADER_GRIPPER_POSITION_CLOSE = 0.01244/' /root/interbotix_ws/src/aloha/aloha_scripts/robot_utils.py &&     sed -i 's/FOLLOWER_GRIPPER_POSITION_OPEN = 0.0579/FOLLOWER_GRIPPER_POSITION_OPEN = 0.05800/' /root/interbotix_ws/src/aloha/aloha_scripts/robot_utils.py &&     sed -i 's/FOLLOWER_GRIPPER_POSITION_CLOSE = 0.0440/FOLLOWER_GRIPPER_POSITION_CLOSE = 0.01844/' /root/interbotix_ws/src/aloha/aloha_scripts/robot_utils.py
```

5. ä¿®æ”¹ `realsense_publisher.py` ä¸­ç›¸æœºåºåˆ—å·ï¼š
```python
camera_names = ['cam_left_wrist', 'cam_high', 'cam_low', 'cam_right_wrist']
camera_sns = ['427622271497', '427622272971', '427622271439', '427622270353']
```

6. æ„å»ºå¹¶è¿è¡Œï¼š
```bash
docker compose -f examples/aloha_real/compose.yml up --build
```

---

## æ ‡å®šå¤¹çˆªä½ç½®ï¼ˆROSï¼‰

```bash
roslaunch interbotix_xsarm_control xsarm_control.launch   robot_model:=vx300s   robot_name:=puppet_right   port:=/dev/ttyDXL_puppet_right   use_rviz:=false   use_joint_pub_gui:=false

# è·å–å…³èŠ‚çŠ¶æ€
rosservice call /puppet_right/get_joint_states

# é™é€Ÿè¯é¢˜ï¼ˆå‡é¢‘ï¼‰
rosrun topic_tools throttle messages /puppet_right/joint_states 1.0 /puppet_right/joint_states_slow

# è§‚å¯Ÿå…³èŠ‚å€¼ï¼ˆç”¨äºæ‰‹åŠ¨æ ‡å®šï¼‰
rostopic echo /puppet_right/joint_states_slow | grep position
```

---

## å¸¸è§é—®é¢˜ï¼ˆFAQï¼‰

**Q1 â€” `pi05_base` ç›®å½•ä¸ºç©ºï¼Œæ— æ³•ä¸‹è½½åŸºç¡€å‚æ•°**  
**è§£æ³•**ï¼šä½¿ç”¨ Google Cloud SDK + `gsutil` ä¸‹è½½ï¼ˆå…ˆå®‰è£… `crcmod`ï¼‰ï¼š
```bash
curl https://sdk.cloud.google.com | bash
sudo apt-get install python3-dev gcc -y
pip install -U crcmod
gsutil -m cp -r gs://openpi-assets/checkpoints/pi05_base ./
```

**Q2 â€” `RevisionNotFoundError: Your dataset must be tagged with a codebase version.`**  
**è§£æ³•**ï¼šä¸º dataset åˆ›å»º tagï¼š
```bash
hf repo tag create <user>/<dataset> v1 --repo-type dataset
```

**Q3 â€” `compute_norm_stats.py` æŠ¥ FileNotFoundError / info.json ç¼ºå¤±**  
**è§£æ³•**ï¼šæ‰‹åŠ¨ä¸‹è½½ dataset åˆ° `/root/.cache/huggingface/lerobot/<user>/<dataset>`ã€‚

**Q4 â€” è®­ç»ƒéœ€è¦ 2.0 æ•°æ®æ ¼å¼ï¼Œä½†ç°æœ‰æ˜¯ 1.0ï¼Œè½¬æ¢æ—¶è¿›ç¨‹è¢«ç»ˆæ­¢**  
**è§£æ³•**ï¼šæ£€æŸ¥æ—¥å¿—ã€å†…å­˜ã€ç£ç›˜ç©ºé—´æˆ–ä½¿ç”¨åˆ†æ­¥è½¬æ¢è„šæœ¬ã€‚

**Q5 â€” uv è­¦å‘Šï¼š`tool.uv.dev-dependencies` å·²å¼ƒç”¨**  
**è§£æ³•**ï¼šæš‚å¯å¿½ç•¥æˆ–è¿ç§»åˆ° `dependency-groups.dev`ã€‚

**Q6 â€” Docker build æŠ¥é”™ï¼ˆç½‘ç»œ/ sudo ï¼‰**  
**è§£æ³•**ï¼šç”¨ sed åˆ é™¤ sudo å¹¶æ£€æŸ¥ apt æºã€‚

---

## æœ€åæ£€æŸ¥æ¸…å•

- [ ] Hugging Face token å·²ç™»å½•  
- [ ] Dataset repo ç»“æ„æ­£ç¡®ï¼ˆæ ¹ç›®å½•ç›´æ¥åŒ…å« `data/` ä¸ `meta/`ï¼‰  
- [ ] å·²æ·»åŠ  dataset tag  
- [ ] assets ä¸ params è·¯å¾„åœ¨ Host / Client å¯è¯»å†™  
- [ ] Colab checkpoint å·²å¤‡ä»½  
- [ ] Host æ˜¾å­˜ â‰¥ 12GB  
- [ ] Client Dockerfile å·²ä¿®å¤ sudo ä¸æ·»åŠ  udev  
- [ ] è‹¥ä½¿ç”¨ GCS ä¸‹è½½ï¼Œç¡®ä¿å·²å®‰è£… crcmod

---
